{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Tt4pKVXp4qff",
    "outputId": "12316683-fa0a-4424-e7ec-3e58a611870a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aboumada/anaconda3/envs/torch_tf/lib/python3.7/site-packages/treetaggerwrapper.py:740: FutureWarning: Possible nested set at position 8\n",
      "  re.IGNORECASE | re.VERBOSE)\n",
      "/home/aboumada/anaconda3/envs/torch_tf/lib/python3.7/site-packages/treetaggerwrapper.py:2044: FutureWarning: Possible nested set at position 152\n",
      "  re.VERBOSE | re.IGNORECASE)\n",
      "/home/aboumada/anaconda3/envs/torch_tf/lib/python3.7/site-packages/treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "/home/aboumada/anaconda3/envs/torch_tf/lib/python3.7/site-packages/treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel,AutoTokenizer, FlaubertModel, BertForSequenceClassification , FlaubertForSequenceClassification\n",
    "from transformers.modeling_utils import SequenceSummary\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import sys\n",
    "import re\n",
    "from easy_nlp.models import ThreeTaskLearning\n",
    "from easy_nlp.training import Train\n",
    "from easy_nlp.preprocessing import TextPreprocessing\n",
    "from easy_nlp.feature_extraction import MetaFeaturesExtraction\n",
    "from easy_nlp.data_visualisation import WordCloudMaker\n",
    "from transformers import  AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "i6mrcvEIjOBU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "90MXwp2sY4pn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "RDTtwB7behWr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ffqI9jMbmrOD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "DYwfgvAe38PL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "nyMnRpgE5lOh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "z1mzN2QD5Jv8",
    "outputId": "1740aa01-2243-4ffa-bc86-37c305663d68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12826"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/data/aboumada/Data/3_Datasets/full_df_noFeatures2.csv\")\n",
    "#df_en = pd.read_csv('/data/aboumada/Data/3_Datasets/anonym_final_en_1.csv')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JPmMynR8ehXE",
    "outputId": "d72b6ea3-fcbd-4f3c-e586-db8c6f2c591f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"params = torch.load('model.pth', map_location=lambda storage, loc: storage)\\nmodel = NonlinearModel(FlaubertModel.from_pretrained('flaubert-base-cased'),device=torch.device('cuda'),n_class=0.1,dropout_rate=0.1)\\nmodel.load_state_dict(params['state_dict'])\\nmodel.cuda()\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''params = torch.load('model.pth', map_location=lambda storage, loc: storage)\n",
    "model = NonlinearModel(FlaubertModel.from_pretrained('flaubert-base-cased'),device=torch.device('cuda'),n_class=0.1,dropout_rate=0.1)\n",
    "model.load_state_dict(params['state_dict'])\n",
    "model.cuda()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 847
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "PNP1f-OrehXK",
    "outputId": "24902c98-68b5-492f-9ee0-8509eddc17b6"
   },
   "source": [
    "## Run if Random Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train , df_test = train_test_split(df,random_state=1, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run if Out of event "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#events_test= ['Bruno', 'Eleanor']\n",
    "#df_test = df[df.event.isin(events_test)]\n",
    "#df_train = df[~df.event.isin(events_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Run if Out of Type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#events_test= ['EffondrementMarseille']\n",
    "#df_train=df_en[['text_clean','CAT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from easy_nlp.preprocessing import TextPreprocessing\n",
    "tp=TextPreprocessing(df,'TEXT')\n",
    "tp.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "lmxzSGxF2y8u",
    "outputId": "320825e1-75c6-41fc-cb3f-df9968588952"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAT\n",
       "AutresMessages            668\n",
       "Avertissement-conseil    1050\n",
       "Critiques                  91\n",
       "Degats-Humains            192\n",
       "Degats-Materiels          396\n",
       "Message-NonUtilisable    7400\n",
       "Soutiens                  383\n",
       "Name: text_clean, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('CAT')['text_clean'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "9sctUMZj3y8Z",
    "outputId": "3e78bdad-f240-415a-d115-e2e910a0f030"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CAT\n",
       "AutresMessages            668\n",
       "Avertissement-conseil    1050\n",
       "Critiques                  91\n",
       "Degats-Humains            192\n",
       "Degats-Materiels          396\n",
       "Message-NonUtilisable    7400\n",
       "Soutiens                  383\n",
       "Name: text_clean, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('CAT')['text_clean'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "2uM49TtI6LLN",
    "outputId": "2a4123e9-9923-4fd8-85b4-dde5c329ad10"
   },
   "outputs": [],
   "source": [
    "def get_sentences_labels(df,text_column='text_clean',label_column='CAT'):    \n",
    "    dic_labels_to_cat = {value:x for x,value in enumerate(df[label_column].unique())}\n",
    "    dic_cat_labels = {x:value for x,value in enumerate(df[label_column].unique())}\n",
    "    df['text_clean']= df['text_clean'].map(lambda text_clean : re.sub('[\"#$%&()*+,-./:;<=>@[\\]^_`{|}~\\n\\t’\\']', '', text_clean))\n",
    "    df2 = df[label_column].map(dic_labels_to_cat)\n",
    "    sentences = df[text_column].values\n",
    "    labels = df2.values.astype(int)\n",
    "    return sentences,labels,dic_cat_labels\n",
    "\n",
    "def custom_sentences_labels(df,dic_cat,dic_cat3,dic_cat2,text_column='text_clean',label_column='CAT'):\n",
    "    dic_cat_labe = dic_cat\n",
    "    dic_cat_labe2 = dic_cat2\n",
    "    dic_cat_labe3 = dic_cat3\n",
    "    #df['text_clean']= df['text_clean'].map(lambda text_clean : re.sub('[\"#$%&()*+,-./:;<=>@[\\]^_`{|}~\\n\\t’\\']', '', text_clean))\n",
    "    dic_labels_to_cat = {v: k for k, v in dic_cat_labe.items()}\n",
    "    dic_labels_to_cat2 = {v: k for k, v in dic_cat_labe2.items()}\n",
    "    dic_labels_to_cat3 = {v: k for k, v in dic_cat_labe3.items()}\n",
    "    sentences = df[text_column].values\n",
    "    df_cat = df['CAT'].map(dic_labels_to_cat)\n",
    "    labels_CAT = df_cat.values.astype(int)\n",
    "    df_cat2 = df['CAT2'].map(dic_labels_to_cat2)\n",
    "    labels_CAT2 = df_cat2.values.astype(int)\n",
    "    df_cat3 = df['CAT3'].map(dic_labels_to_cat3)\n",
    "    labels_CAT3 = df_cat3.values.astype(int)\n",
    "\n",
    "    return sentences,(labels_CAT,labels_CAT3,labels_CAT2),dic_cat_labe\n",
    "\n",
    "dic_cat_labels_CAT = {0: 'Soutiens', 1: 'Message-NonUtilisable', 2: 'Degats-Humains', 3: 'Degats-Materiels', 4: 'Avertissement-conseil', 5: 'AutresMessages',6: 'Critiques'}\n",
    "dic_cat_labels_CAT3 = {0: 'Message-InfoUrgent', 1: 'Message-NonUtilisable', 2: 'Message-InfoNonUrgent'}\n",
    "dic_cat_labels_CAT2 = {0: 'Message-Utilisable', 1: 'Message-NonUtilisable'}\n",
    "\n",
    "#sentences_train,labels_train,dic_cat_labels=get_sentences_labels(df_train,text_column='text_clean',label_column='CAT')\n",
    "sentences_train,labels_train,_=custom_sentences_labels(df_train,dic_cat_labels_CAT,dic_cat_labels_CAT3,dic_cat_labels_CAT2,text_column='text_clean',label_column='CAT')\n",
    "sentences_test,test_labels,_=custom_sentences_labels(df_test,dic_cat_labels_CAT,dic_cat_labels_CAT3,dic_cat_labels_CAT2,text_column='text_clean',label_column='CAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251,
     "referenced_widgets": [
      "f72eddb948294b7bb00463d4ed6cc5af",
      "a0d22be9c5af47fba9826f45cb404d14",
      "d6a4103ed28c441aaece9f8f9f11c8b5",
      "60831d0d88144f8aa18798597bf1ac94",
      "2024e77c10704e18bbb473afa0f5a908",
      "94aca90ab4d843b3a2e9e067a2b92395",
      "16aebfb576bc463db97e5a222b9f264f",
      "0c2259b7d4ad4020ab30a1a849287ccf",
      "846e2eb2279a47a39ab28c47fc60ab54",
      "3744741d6d7c478ea51f4ed86cfcd437",
      "ce9418995bda46fc9efd0f24996e8167",
      "44175059856346149d08ed289d4179c3",
      "e9819ed7b8b74bc4ab9c31edb3a8caae",
      "0d362c6a6c9245f4815c934ccaf1d717",
      "51295ca06b7b457baeb45d11c6e77b09",
      "7b3b7019567541cf99bfb59c3780325c"
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "fU3z3D5hNwnj",
    "outputId": "a21df7bc-f772-4ae0-a018-e7345d522bb1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from easy_nlp.feature_extraction import BertInput\n",
    "\n",
    "bert_input= BertInput(AutoTokenizer.from_pretrained('flaubert/flaubert_base_cased'))\n",
    "\n",
    "\n",
    "X_train = bert_input.fit_transform(sentences_train)\n",
    "X_test = bert_input.fit_transform(sentences_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "2qWr_SoAvjyD"
   },
   "outputs": [],
   "source": [
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels_cat, validation_labels_cat,train_labels_cat3, validation_labels_cat3,train_labels_cat2, validation_labels_cat2 = train_test_split(X_train[0], labels_train[0], labels_train[1], labels_train[2],\n",
    "                                                            random_state=1, test_size=0.2)\n",
    "# Do the same for the masks.\n",
    "train_masks, validation_masks= train_test_split(X_train[1],\n",
    "                                             random_state=1, test_size=0.2)\n",
    "\n",
    "test_inputs = X_test[0]\n",
    "test_masks = X_test[1]\n",
    "test_labels_cat = test_labels[0]\n",
    "test_labels_cat3 = test_labels[1]\n",
    "test_labels_cat2 = test_labels[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "e9EtZELWvQNq",
    "outputId": "5144d70e-584b-4e56-c27f-9dcd0945ab49"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5MHONo0BEwRR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "QLbLpNo1EyRX"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "# Convert all inputs and labels into torch tensors, the required datatype \n",
    "# for our model.\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "\n",
    "train_labels_cat = torch.tensor(train_labels_cat)\n",
    "validation_labels_cat = torch.tensor(validation_labels_cat)\n",
    "test_labels_cat = torch.tensor(test_labels_cat)\n",
    "\n",
    "train_labels_cat3 = torch.tensor(train_labels_cat3)\n",
    "validation_labels_cat3 = torch.tensor(validation_labels_cat3)\n",
    "test_labels_cat3 = torch.tensor(test_labels_cat3)\n",
    "\n",
    "train_labels_cat2 = torch.tensor(train_labels_cat2)\n",
    "validation_labels_cat2 = torch.tensor(validation_labels_cat2)\n",
    "test_labels_cat2 = torch.tensor(test_labels_cat2)\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "YCqZsxQG8qkL",
    "outputId": "aa567bec-4ba5-4be6-ba15-8c11c20178cc"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "N44YvMDrowB7"
   },
   "outputs": [],
   "source": [
    "def get_label_callback(dataset,idx):\n",
    "    return dataset[idx][3].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EplrI9njoxJl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "iIawUkP_popj",
    "outputId": "ef5231b9-97df-4c4e-e6e3-7242d4675162"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "aDJeM2k1E02U"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs,train_masks,train_labels_cat,train_labels_cat3,train_labels_cat2)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "#train_sampler = ImbalancedDatasetSampler(train_data,callback_get_label=get_label_callback)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs,validation_masks ,validation_labels_cat,validation_labels_cat3,validation_labels_cat2)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Create the DataLoader for our test set.\n",
    "test_data = TensorDataset(test_inputs,test_masks, test_labels_cat,test_labels_cat3,test_labels_cat2)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dpLbwoAApnxJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ql6VZYido-w_"
   },
   "outputs": [],
   "source": [
    "#model =FlaubertBertForSequenceClassification.from_pretrained('bert_fine_tuned_all_fr',num_labels = 7)\n",
    "#model.bert.embeddings.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4f9cae1d99f3443fb98b00a4f7e682cf",
      "ff14b9e555c54d7c89c1dbbff543b47b",
      "77980552d8a344929721990a95e75f95",
      "06a769832535409e90bebadf4385bf56",
      "d73a2248948b4a8ba64d0c61d3c4a0e7",
      "ef456ebc1a3f459d8db57ffa12f509bd",
      "f6aadf099bc54f279d31aacca1e7ed89",
      "065edcd156034674802afe441b506472",
      "8e96be71b679486e89215ca965df9e9b",
      "273dc786cb2c49f29b2acb292eba3b9b",
      "fe09e9ce448e47f5baf8074e72c8f99f",
      "795050feb4c9465c80729c5184dbac43",
      "652b303e2fb648069359c59e57ff5367",
      "8cb852ff447a4dd5b106bf78f46fa11f",
      "bd9b36bf1b7040d59206b02525d8daaf",
      "c8615680e0404a47b76b3e9cf2c6b395"
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q-w7gOSsE3gg",
    "outputId": "36f4257e-2451-4b4a-bee9-81631d68e23f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThreeTaskLearning(\n",
       "  (bert): FlaubertModel(\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (embeddings): Embedding(68729, 768, padding_idx=2)\n",
       "    (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (attentions): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm1): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (ffns): ModuleList(\n",
       "      (0): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm2): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (linear_cat): Linear(in_features=768, out_features=7, bias=True)\n",
       "  (linear_cat3): Linear(in_features=768, out_features=3, bias=True)\n",
       "  (linear_cat2): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (activation): LeakyReLU(negative_slope=0.01)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ThreeTaskLearning(model=FlaubertModel.from_pretrained('flaubert-base-cased'),\n",
    "        dropout_rate = 0.1, \n",
    "        device = torch.device(\"cuda\"),\n",
    "        )\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4OzL-JucAhxR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,recall_score,precision_score\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Function to calculate the f1_score of our predictions vs labels\n",
    "def flat_f1(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, pred_flat, average='macro')\n",
    "\n",
    "def flat_recall(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return recall_score(labels_flat, pred_flat, average='macro')\n",
    "\n",
    "def flat_precision(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return precision_score(labels_flat, pred_flat, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iA51FWSoS5na"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def train(model,train_dataloader,validation_dataloader):\n",
    "    # Set the seed value all ovr the place to make this reproducible.\n",
    "    seed_val = 2\n",
    "\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "    model_save_path='tmp'\n",
    "    loss_values = []\n",
    "    hist_valid_scores=[]\n",
    "    \n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "        logs = {}\n",
    "\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_loss = 0\n",
    "        total_accuracy=0\n",
    "\n",
    "        # Put the model into training mode. Don't be mislead--the call to \n",
    "        # `train` just changes the *mode*, it doesn't *perform* the training.\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "            # Progress update every 40 batches.\n",
    "            if step % 40 == 0 and not step == 0:\n",
    "                # Calculate elapsed time in minutes.\n",
    "                elapsed = format_time(time.time() - t0)\n",
    "\n",
    "                # Report progress.\n",
    "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "            # Unpack this training batch from our dataloader. \n",
    "            #\n",
    "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
    "            # `to` method.\n",
    "            #\n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: input ids \n",
    "            #   [1]: attention masks\n",
    "            #   [2]: labels \n",
    "\n",
    "\n",
    "            # Always clear any previously calculated gradients before performing a\n",
    "            # backward pass. PyTorch doesn't do this automatically because \n",
    "            # accumulating the gradients is \"convenient while training RNNs\". \n",
    "            # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
    "            model.zero_grad()        \n",
    "\n",
    "            # Perform a forward pass (evaluate the model on this training batch).\n",
    "            # This will return the loss (rather than the model output) because we\n",
    "            # have provided the `labels`.\n",
    "            # The documentation for this `model` function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            outputs = model(batch)\n",
    "\n",
    "            # The call to `model` always returns a tuple, so we need to pull the \n",
    "            # loss value out of the tuple.\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[3]\n",
    "            #print(logits)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = batch[4]\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences.\n",
    "            total_accuracy += flat_accuracy(logits, label_ids)\n",
    "            # Accumulate the training loss over all of the batches so that we can\n",
    "            # calculate the average loss at the end. `loss` is a Tensor containing a\n",
    "            # single value; the `.item()` function just returns the Python value \n",
    "            # from the tensor.\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate the gradients.\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0.\n",
    "            # This is to help prevent the \"exploding gradients\" problem.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient.\n",
    "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
    "            # modified based on their gradients, the learning rate, etc.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)  \n",
    "        avg_train_accuracy = total_accuracy /len(train_dataloader)\n",
    "        # Store the loss value for plotting the learning curve.\n",
    "        loss_values.append(avg_train_loss)\n",
    "        logs[\"log loss\"] = avg_train_loss\n",
    "        logs[\"accuracy\"] = avg_train_accuracy\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "        print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        print(\"\")\n",
    "        print(\"Running Validation...\")\n",
    "\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Put the model in evaluation mode--the dropout layers behave differently\n",
    "        # during evaluation.\n",
    "        model.eval()\n",
    "\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy,eval_f1,eval_recall,eval_precesion = 0, 0 , 0,0,0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        \n",
    "        validation_loss=0\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            \n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.to(torch.device('cuda')) for t in batch)\n",
    "\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_labels_cat,b_labels_cat3,b_labels_cat2 = batch\n",
    "\n",
    "            # Telling the model not to compute or store gradients, saving memory and\n",
    "            # speeding up validation\n",
    "            with torch.no_grad():        \n",
    "\n",
    "                # Forward pass, calculate logit predictions.\n",
    "                # This will return the logits rather than the loss because we have\n",
    "                # not provided labels.\n",
    "                # token_type_ids is the same as the \"segment ids\", which \n",
    "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "                # The documentation for this `model` function is here: \n",
    "                # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "                outputs = model(batch)\n",
    "\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            validation_loss += outputs[0]\n",
    "            logits = outputs[3]\n",
    "            #print(logits)\n",
    "        \n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels_cat2.to('cpu').numpy()\n",
    "\n",
    "            # Calculate the accuracy for this batch of test sentences.\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "            tmp_eval_f1 = flat_f1(logits,label_ids)\n",
    "            tmp_eval_recall = flat_recall(logits,label_ids)\n",
    "            tmp_eval_precision = flat_precision(logits,label_ids)\n",
    "            # Accumulate the total scores.\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            eval_f1 += tmp_eval_f1\n",
    "            eval_recall +=tmp_eval_recall\n",
    "            eval_precesion +=tmp_eval_precision\n",
    "\n",
    "            # Track the number of batches\n",
    "            nb_eval_steps += 1\n",
    "        validation_loss=validation_loss/len(validation_dataloader)\n",
    "        is_better = len(hist_valid_scores) == 0 or validation_loss < min(hist_valid_scores)\n",
    "        hist_valid_scores.append(validation_loss)\n",
    "\n",
    "        if is_better:\n",
    "          patience = 0\n",
    "          print('save currently the best model to [%s]' % model_save_path, file=sys.stderr)\n",
    "          model.save(model_save_path)\n",
    "          # also save the optimizers' state\n",
    "          torch.save(optimizer.state_dict(), model_save_path + '.optim')\n",
    "        elif patience < 5:\n",
    "          patience += 1\n",
    "          '''print('hit patience %d' % patience, file=sys.stderr)\n",
    "          if patience == int(5):\n",
    "            # decay lr, and restore from previously best checkpoint\n",
    "            print('load previously best model and decay learning rate to ', file=sys.stderr)\n",
    "            # load model\n",
    "            params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n",
    "            model.load_state_dict(params['state_dict'])\n",
    "            model = model.to(torch.device(\"cuda\"))\n",
    "            print('restore parameters of the optimizers', file=sys.stderr)\n",
    "            optimizer.load_state_dict(torch.load(model_save_path + '.optim'))\n",
    "            # reset patience\n",
    "            patience = 0'''\n",
    "        # Report the final accuracy for this validation run.\n",
    "        print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "        print(\"  F1: {0:.2f}\".format(eval_f1/nb_eval_steps))\n",
    "        print(\"  Recall: {0:.2f}\".format(eval_recall/nb_eval_steps))\n",
    "        print(\"  Precision: {0:.2f}\".format(eval_precesion/nb_eval_steps))\n",
    "        print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "\n",
    "    # load model\n",
    "\n",
    "    # reset patience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "W3pKjPg9Ivds"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW,get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 3e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 1\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs \n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yhDsewdSehZD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qIABJxZ-19ii"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "qZAsjKnKI4Rx",
    "outputId": "b10dfd2d-35fb-47c6-9983-4b7a4cf615f1",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Train' object has no attribute 'fit_ThreeTask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-c56e0a5ca63a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_ThreeTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Train' object has no attribute 'fit_ThreeTask'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "train = Train()\n",
    "train.fit_ThreeTask(model,train_dataloader,validation_dataloader,epochs,torch.device('cuda'),optimizer)\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "KzmtPO6OJbcG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 161 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_dataloader)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Tracking variables \n",
    "predictions_cat,predictions_cat3,predictions_cat2 , true_labels_cat,true_labels_cat3,true_labels_cat2  = [], [],[],[],[],[]\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(torch.device(\"cuda\")) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask,b_labels_cat,b_labels_cat3,b_labels_cat2 = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model((b_input_ids,b_input_mask))\n",
    "        logits_cat = outputs[0]\n",
    "        logits_cat3 = outputs[1]\n",
    "        logits_cat2 = outputs[2]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits_cat = logits_cat.detach().cpu().numpy()\n",
    "    label_ids_cat = b_labels_cat.to('cpu').numpy()\n",
    "    predictions_cat.extend(logits_cat)\n",
    "    true_labels_cat.extend(label_ids_cat)\n",
    "    \n",
    "    logits_cat3 = logits_cat3.detach().cpu().numpy()\n",
    "    label_ids_cat3 = b_labels_cat3.to('cpu').numpy()\n",
    "    predictions_cat3.extend(logits_cat3)\n",
    "    true_labels_cat3.extend(label_ids_cat3)\n",
    "    \n",
    "    \n",
    "    logits_cat2 = logits_cat2.detach().cpu().numpy()\n",
    "    label_ids_cat2 = b_labels_cat2.to('cpu').numpy()\n",
    "    predictions_cat2.extend(logits_cat2)\n",
    "    true_labels_cat2.extend(label_ids_cat2)\n",
    "\n",
    "\n",
    "print('    DONE.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "n44qF6NPKHcv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8203429462197973\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "       AutresMessages     0.5000    0.3034    0.3776       178\n",
      "Avertissement-conseil     0.7440    0.7340    0.7390       297\n",
      "            Critiques     0.0000    0.0000    0.0000        28\n",
      "       Degats-Humains     0.5102    0.5102    0.5102        49\n",
      "     Degats-Materiels     0.7414    0.4624    0.5695        93\n",
      "Message-NonUtilisable     0.8604    0.9310    0.8943      1827\n",
      "             Soutiens     0.7901    0.6809    0.7314        94\n",
      "\n",
      "             accuracy                         0.8203      2566\n",
      "            macro avg     0.5923    0.5174    0.5460      2566\n",
      "         weighted avg     0.7990    0.8203    0.8057      2566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred_flat_cat = np.argmax(predictions_cat, axis=1)\n",
    "true_labels_cat=[dic_cat_labels_CAT.get(x) for x in true_labels_cat]\n",
    "pred_flat_cat = [dic_cat_labels_CAT.get(x) for x in pred_flat_cat]\n",
    "\n",
    "\n",
    "cr= classification_report(true_labels_cat,pred_flat_cat,digits=4)\n",
    "print(accuracy_score(pred_flat_cat,true_labels_cat))\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "jF21fm-zehZd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8320342946219798\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "Message-InfoNonUrgent     0.6405    0.5167    0.5720       300\n",
      "   Message-InfoUrgent     0.7092    0.7221    0.7156       439\n",
      "Message-NonUtilisable     0.8860    0.9102    0.8979      1827\n",
      "\n",
      "             accuracy                         0.8320      2566\n",
      "            macro avg     0.7452    0.7163    0.7285      2566\n",
      "         weighted avg     0.8270    0.8320    0.8286      2566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred_flat_cat3 = np.argmax(predictions_cat3, axis=1)\n",
    "true_labels_cat3 =[dic_cat_labels_CAT3.get(x) for x in true_labels_cat3]\n",
    "pred_flat_cat3 = [dic_cat_labels_CAT3.get(x) for x in pred_flat_cat3]\n",
    "\n",
    "\n",
    "cr= classification_report(true_labels_cat3,pred_flat_cat3,digits=4)\n",
    "print(accuracy_score(pred_flat_cat3,true_labels_cat3))\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "HEIPOlO8ehZh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8542478565861262\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "Message-NonUtilisable     0.9025    0.8916    0.8970      1827\n",
      "   Message-Utilisable     0.7398    0.7618    0.7507       739\n",
      "\n",
      "             accuracy                         0.8542      2566\n",
      "            macro avg     0.8212    0.8267    0.8238      2566\n",
      "         weighted avg     0.8556    0.8542    0.8549      2566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred_flat_cat2 = np.argmax(predictions_cat2, axis=1)\n",
    "true_labels_cat2 =[dic_cat_labels_CAT2.get(x) for x in true_labels_cat2]\n",
    "pred_flat_cat2 = [dic_cat_labels_CAT2.get(x) for x in pred_flat_cat2]\n",
    "\n",
    "\n",
    "cr= classification_report(true_labels_cat2,pred_flat_cat2,digits=4)\n",
    "print(accuracy_score(pred_flat_cat2,true_labels_cat2))\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "MxFV4TGAehZl"
   },
   "outputs": [],
   "source": [
    "model.save('model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "60S-GzCfehZp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_to_cat = {v:k for k,v in dic_cat_labels_CAT.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "liste = []\n",
    "for _,row in df_test.iterrows():\n",
    "    x_input = []\n",
    "    x_input = berts_input([row['text_clean']],FlaubertTokenizer.from_pretrained('flaubert_fine_tuned_alldata'))\n",
    "    x_input = torch.tensor(x_input)\n",
    "    x_input = x_input.to(torch.device('cuda'))\n",
    "    output = model(x_input)\n",
    "    output[0].cpu\n",
    "    output[0].cpu()\n",
    "    pred_flat = np.argmax(output[0].cpu().detach().numpy())\n",
    "    if pred_flat!=name_to_cat[row['CAT']]:\n",
    "        print('Annotation human:',row['CAT'])\n",
    "        print('Annotation Machine:',dic_cat_labels_CAT[pred_flat] )\n",
    "        print(row['TEXT'])\n",
    "        liste.append([row['TEXT'],row['CAT'],dic_cat_labels_CAT[pred_flat]])\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_resultat = pd.DataFrame(liste,columns=['Text','Human annotation','Machine Annotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_resultat.to_csv('error.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "YbFw5Bf6ehZv"
   },
   "outputs": [],
   "source": [
    "x_input = torch.tensor(x_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "yDaGb1nWQ524"
   },
   "outputs": [],
   "source": [
    "# Three classes\n",
    "x_input = x_input.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "KQSp-xzlehZ5"
   },
   "outputs": [],
   "source": [
    "output = model(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "AQ2VdzKyRVzv"
   },
   "outputs": [],
   "source": [
    "output[0].cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "eS08J490RpG4"
   },
   "outputs": [],
   "source": [
    "pred_flat = np.argmax(output[0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "-koMpoCqehaH"
   },
   "outputs": [],
   "source": [
    "pred_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "2br-uW8pehaN"
   },
   "outputs": [],
   "source": [
    "x=df_test['text_clean'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kqT-7s0ehaS"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "xquzubrQehaU"
   },
   "outputs": [],
   "source": [
    "max_len = max(len(s) for s in sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "WaNyf9f5ehaZ"
   },
   "outputs": [],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "BmEQaWevehae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "6xSARQ_aehak"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mU_CQrvaehap"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "XDXm44CXehas"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "O8vvdluSehav"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "tyl43ZmLehax"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QNI95yNKeha1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Z3-BA_vfeha4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WnvSgbO0eha7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "LuWeeaeEeha9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DaB8pXo6ehbC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2u4UInmFehbG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Y1_tjR61ehbK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1ObqrYFkehbN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AVpM54y3ehbQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "B3UFEv2cehbT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Rr60uzFcehbW"
   },
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Flaubert_Multitask+learning+All.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "torch_tf_env",
   "language": "python",
   "name": "torch_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
