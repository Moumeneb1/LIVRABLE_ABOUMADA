{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Tt4pKVXp4qff",
    "outputId": "12316683-fa0a-4424-e7ec-3e58a611870a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aboumada/anaconda3/envs/torch_tf/lib/python3.7/site-packages/treetaggerwrapper.py:740: FutureWarning: Possible nested set at position 8\n",
      "  re.IGNORECASE | re.VERBOSE)\n",
      "/home/aboumada/anaconda3/envs/torch_tf/lib/python3.7/site-packages/treetaggerwrapper.py:2044: FutureWarning: Possible nested set at position 152\n",
      "  re.VERBOSE | re.IGNORECASE)\n",
      "/home/aboumada/anaconda3/envs/torch_tf/lib/python3.7/site-packages/treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "/home/aboumada/anaconda3/envs/torch_tf/lib/python3.7/site-packages/treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel,BertTokenizer,FlaubertTokenizer, FlaubertModel,AutoTokenizer, BertForSequenceClassification , FlaubertForSequenceClassification\n",
    "from transformers.modeling_utils import SequenceSummary\n",
    "from tensorboardX import SummaryWriter\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import sys\n",
    "import re\n",
    "from models import BasicBertForClassification\n",
    "from train import train_noFeatures\n",
    "from preprocessing.text_preprocessing import TextPreprocessing\n",
    "from preprocessing.feature_enginering import FeaturesExtraction\n",
    "from transformers import  AutoModel\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FocalLoss2(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n",
    "    'Focal Loss for Dense Object Detection. (https://arxiv.org/abs/1708.02002)'\n",
    "        Focal_Loss= -1*alpha*(1-pt)*log(pt)\n",
    "\n",
    "    Params:\n",
    "        :param num_class:\n",
    "        :param alpha: (tensor) 3D or 4D the scalar factor for this criterion\n",
    "        :param gamma: (float,double) gamma > 0 reduces the relative loss for well-classified examples (p>0.5) putting more\n",
    "                        focus on hard misclassified example\n",
    "        :param smooth: (float,double) smooth value when cross entropy\n",
    "        :param balance_index: (int) balance class index, should be specific when alpha is float\n",
    "        :param size_average: (bool, optional) By default, the losses are averaged over each loss element in the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_class, alpha=None, gamma=1, balance_index=-1, smooth=None, size_average=False):\n",
    "        super(FocalLoss2, self).__init__()\n",
    "        self.num_class = num_class\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.smooth = smooth\n",
    "        self.size_average = size_average\n",
    "\n",
    "        if self.alpha is None:\n",
    "            self.alpha = torch.ones(self.num_class, 1)\n",
    "        elif isinstance(self.alpha, (list, np.ndarray)):\n",
    "            assert len(self.alpha) == self.num_class\n",
    "            self.alpha = torch.FloatTensor(alpha).view(self.num_class, 1)\n",
    "            self.alpha = self.alpha / self.alpha.sum()\n",
    "        elif isinstance(self.alpha, float):\n",
    "            alpha = torch.ones(self.num_class, 1)\n",
    "            alpha = alpha * (1 - self.alpha)\n",
    "            alpha[balance_index] = self.alpha\n",
    "            self.alpha = alpha\n",
    "        else:\n",
    "            raise TypeError('Not support alpha type')\n",
    "\n",
    "        if self.smooth is not None:\n",
    "            if self.smooth < 0 or self.smooth > 1.0:\n",
    "                raise ValueError('smooth value should be in [0,1]')\n",
    "\n",
    "    def forward(self, logit, target):\n",
    "\n",
    "        #logit = F.softmax(input, dim=1)\n",
    "        logit=torch.nn.functional.softmax(logit,dim=1)\n",
    "        if logit.dim() > 2:\n",
    "            # N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n",
    "            logit = logit.view(logit.size(0), logit.size(1), -1)\n",
    "            logit = logit.permute(0, 2, 1).contiguous()\n",
    "            logit = logit.view(-1, logit.size(-1))\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        # N = input.size(0)\n",
    "        # alpha = torch.ones(N, self.num_class)\n",
    "        # alpha = alpha * (1 - self.alpha)\n",
    "        # alpha = alpha.scatter_(1, target.long(), self.alpha)\n",
    "        epsilon = 1e-6\n",
    "        alpha = self.alpha.to(logit.device)\n",
    "\n",
    "        idx = target.cpu().long()\n",
    "\n",
    "        one_hot_key = torch.FloatTensor(target.size(0), self.num_class).zero_()\n",
    "        one_hot_key = one_hot_key.scatter_(1, idx, 1)\n",
    "        one_hot_key = one_hot_key.to(logit.device)\n",
    "\n",
    "        if self.smooth:\n",
    "            one_hot_key = torch.clamp(one_hot_key, self.smooth / (self.num_class - 1), 1.0 - self.smooth)\n",
    "        pt = (one_hot_key * logit).sum(1) + epsilon\n",
    "        logpt = pt.log()\n",
    "\n",
    "        gamma = self.gamma\n",
    "\n",
    "        alpha = alpha[idx]\n",
    "        loss = -1 * alpha * torch.pow((1 - pt), gamma) * logpt\n",
    "\n",
    "        if self.size_average:\n",
    "            loss = loss.mean()\n",
    "        else:\n",
    "            loss = loss.sum()\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "i6mrcvEIjOBU"
   },
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "z1mzN2QD5Jv8",
    "outputId": "1740aa01-2243-4ffa-bc86-37c305663d68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12826"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/data/aboumada/Data/3_Datasets/full_df_noFeatures_Preapred.csv\")\n",
    "#df=df[df.classe2 != 'Poubelle']\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JPmMynR8ehXE",
    "outputId": "d72b6ea3-fcbd-4f3c-e586-db8c6f2c591f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13events_df_noFeatures.csv\r\n",
      "Untitled.ipynb\r\n",
      "Untitled1.ipynb\r\n",
      "all_unlabeled.txt\r\n",
      "anonym_final_en.csv\r\n",
      "anonym_final_en_1.csv\r\n",
      "\u001b[0m\u001b[38;5;27mback\u001b[0m/\r\n",
      "corpus2.csv\r\n",
      "\u001b[38;5;34mcorpus_2non_annotated.csv\u001b[0m*\r\n",
      "corpus_PSY_Features\r\n",
      "corpus_PSY_Features.csv\r\n",
      "corpus_PSY_Preapred_Features.csv\r\n",
      "corpus_PSY_Prepared_Features.csv\r\n",
      "\u001b[38;5;34mcorpus_annote_RepPer - corpus_annote_RepPer.csv\u001b[0m*\r\n",
      "data_clas\r\n",
      "data_clas_export.pkl\r\n",
      "data_lm\r\n",
      "data_lm_export.pkl\r\n",
      "df_train.csv\r\n",
      "duplicates\r\n",
      "duplicates.csv\r\n",
      "final_en_1.csv\r\n",
      "final_en_2.csv\r\n",
      "full_df_Features.csv\r\n",
      "full_df_noFeatures.csv\r\n",
      "full_df_noFeatures2.csv\r\n",
      "full_df_noFeatures_Preapred.csv\r\n",
      "\u001b[38;5;27mg_data\u001b[0m/\r\n",
      "g_data.csv\r\n",
      "\u001b[38;5;27mmodels\u001b[0m/\r\n",
      "moumene testing 2.ipynb\r\n",
      "original_df_test.csv\r\n",
      "original_df_train.csv\r\n",
      "processd_final_en_1.csv\r\n",
      "\u001b[38;5;34mracism-testing-testing_multi_label.csv\u001b[0m*\r\n",
      "\u001b[38;5;34mtous-final_sansdoublon.xml\u001b[0m*\r\n",
      "\u001b[38;5;34mtrain_multi_label.csv\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls /data/aboumada/Data/3_Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_preprocessing = TextPreprocessing(df,\"TEXT\")\n",
    "text_preprocessing.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CAT</th>\n",
       "      <th>ID</th>\n",
       "      <th>event</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>nb_retweets</th>\n",
       "      <th>nb_likes</th>\n",
       "      <th>num_tweets</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>likes</th>\n",
       "      <th>lists</th>\n",
       "      <th>CAT3</th>\n",
       "      <th>CAT2</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Degats-Materiels</td>\n",
       "      <td>1.054066e+18</td>\n",
       "      <td>Aude</td>\n",
       "      <td>Inondations dans l'Aude:dégâts \"de l'ordre de ...</td>\n",
       "      <td>-0.785795</td>\n",
       "      <td>-0.894291</td>\n",
       "      <td>0.956674</td>\n",
       "      <td>1.387893</td>\n",
       "      <td>0.349577</td>\n",
       "      <td>1.078412</td>\n",
       "      <td>-0.657592</td>\n",
       "      <td>Message-InfoUrgent</td>\n",
       "      <td>Message-Utilisable</td>\n",
       "      <td>inondations dans l'aude:dégâts \"de l'ordre de ...</td>\n",
       "      <td>Inondations dans l Aude dégâts de l ordre de n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avertissement-conseil</td>\n",
       "      <td>9.267310e+17</td>\n",
       "      <td>Autre</td>\n",
       "      <td>Département Hérault en vigilance Jaune pluie ️...</td>\n",
       "      <td>1.647024</td>\n",
       "      <td>1.586924</td>\n",
       "      <td>-0.469193</td>\n",
       "      <td>1.005712</td>\n",
       "      <td>0.814903</td>\n",
       "      <td>1.892032</td>\n",
       "      <td>1.030431</td>\n",
       "      <td>Message-InfoUrgent</td>\n",
       "      <td>Message-Utilisable</td>\n",
       "      <td>département hérault en vigilance jaune pluie ️...</td>\n",
       "      <td>Département Hérault en vigilance Jaune pluie ️...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avertissement-conseil</td>\n",
       "      <td>1.054363e+18</td>\n",
       "      <td>Aude</td>\n",
       "      <td>[#CodesCourtage] Inondations tragiques dans l’...</td>\n",
       "      <td>-0.785795</td>\n",
       "      <td>-0.894291</td>\n",
       "      <td>-0.829161</td>\n",
       "      <td>0.098955</td>\n",
       "      <td>0.104824</td>\n",
       "      <td>-0.498565</td>\n",
       "      <td>-0.657592</td>\n",
       "      <td>Message-InfoUrgent</td>\n",
       "      <td>Message-Utilisable</td>\n",
       "      <td>[#codescourtage] inondations tragiques dans l’...</td>\n",
       "      <td>CodesCourtage Inondations tragiques dans l Au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AutresMessages</td>\n",
       "      <td>9.587054e+17</td>\n",
       "      <td>Autre</td>\n",
       "      <td>Merci à @FDSEA77 pour ces photos prises par #d...</td>\n",
       "      <td>1.755725</td>\n",
       "      <td>1.741995</td>\n",
       "      <td>-0.165393</td>\n",
       "      <td>0.863903</td>\n",
       "      <td>1.192329</td>\n",
       "      <td>0.537415</td>\n",
       "      <td>1.030431</td>\n",
       "      <td>Message-InfoNonUrgent</td>\n",
       "      <td>Message-Utilisable</td>\n",
       "      <td>merci à @fdseanumnum pour ces photos prises pa...</td>\n",
       "      <td>Merci à pour ces photos prises par drones qui ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avertissement-conseil</td>\n",
       "      <td>8.014572e+17</td>\n",
       "      <td>Autre</td>\n",
       "      <td>[️ALERTE MÉTÉO⚠️] Vigilance #orange \"orages, p...</td>\n",
       "      <td>1.647024</td>\n",
       "      <td>1.476086</td>\n",
       "      <td>0.560304</td>\n",
       "      <td>0.570291</td>\n",
       "      <td>1.373599</td>\n",
       "      <td>1.748246</td>\n",
       "      <td>-0.657592</td>\n",
       "      <td>Message-InfoUrgent</td>\n",
       "      <td>Message-Utilisable</td>\n",
       "      <td>[️ale e météo⚠️] vigilance #orange \"orages, pl...</td>\n",
       "      <td>️ALERTE MÉTÉO⚠️ Vigilance orange orages pluie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12821</th>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>7.000477e+17</td>\n",
       "      <td>Ulrika</td>\n",
       "      <td>J'aime une vidéo @YouTube de @davidlpokemon - ...</td>\n",
       "      <td>-0.785795</td>\n",
       "      <td>-0.894291</td>\n",
       "      <td>-0.255538</td>\n",
       "      <td>-1.511671</td>\n",
       "      <td>-3.149651</td>\n",
       "      <td>-0.687065</td>\n",
       "      <td>-0.657592</td>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>j'aime une vidéo @youtube de @davidlpokemon - ...</td>\n",
       "      <td>J aime une vidéo de   Ouverture de numero Boos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12822</th>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>6.980546e+17</td>\n",
       "      <td>Ulrika</td>\n",
       "      <td>Ce soir je dors en #Bretagne :-)\\nje dors en B...</td>\n",
       "      <td>-0.785795</td>\n",
       "      <td>-0.894291</td>\n",
       "      <td>-0.858333</td>\n",
       "      <td>1.028186</td>\n",
       "      <td>-0.244243</td>\n",
       "      <td>-0.386106</td>\n",
       "      <td>1.589166</td>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>ce soir je dors en #bretagne :-) je dors en br...</td>\n",
       "      <td>Ce soir je dors en Bretagne je dors en Bretagn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12823</th>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>6.989351e+17</td>\n",
       "      <td>Ulrika</td>\n",
       "      <td>Tempête Ulrika sur le Bassin d'Arcachon Andern...</td>\n",
       "      <td>-0.785795</td>\n",
       "      <td>-0.894291</td>\n",
       "      <td>0.461589</td>\n",
       "      <td>1.187429</td>\n",
       "      <td>-0.015061</td>\n",
       "      <td>-0.138041</td>\n",
       "      <td>-0.657592</td>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>tempête ulrika sur le bassin d'arcachon andern...</td>\n",
       "      <td>Tempête Ulrika sur le Bassin d Arcachon Andern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12824</th>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>6.987924e+17</td>\n",
       "      <td>Ulrika</td>\n",
       "      <td>Mi è piaciuto un video di @YouTube: http://you...</td>\n",
       "      <td>-0.785795</td>\n",
       "      <td>-0.894291</td>\n",
       "      <td>-0.632697</td>\n",
       "      <td>1.381086</td>\n",
       "      <td>-0.168708</td>\n",
       "      <td>-0.173275</td>\n",
       "      <td>-0.657592</td>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>mi è piaciuto un video di @youtube:    tempête...</td>\n",
       "      <td>Mi è piaciuto un video di   Tempête en Mer d I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12825</th>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>6.995217e+17</td>\n",
       "      <td>Ulrika</td>\n",
       "      <td>Bretagne profonde https://youtu.be/jRRLH3IfszQ...</td>\n",
       "      <td>-0.785795</td>\n",
       "      <td>-0.894291</td>\n",
       "      <td>-0.387825</td>\n",
       "      <td>-0.212842</td>\n",
       "      <td>-1.189493</td>\n",
       "      <td>-1.343739</td>\n",
       "      <td>-0.657592</td>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>Message-NonUtilisable</td>\n",
       "      <td>bretagne profonde    via @youtube</td>\n",
       "      <td>Bretagne profonde   via</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12826 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         CAT            ID   event  \\\n",
       "0           Degats-Materiels  1.054066e+18    Aude   \n",
       "1      Avertissement-conseil  9.267310e+17   Autre   \n",
       "2      Avertissement-conseil  1.054363e+18    Aude   \n",
       "3             AutresMessages  9.587054e+17   Autre   \n",
       "4      Avertissement-conseil  8.014572e+17   Autre   \n",
       "...                      ...           ...     ...   \n",
       "12821  Message-NonUtilisable  7.000477e+17  Ulrika   \n",
       "12822  Message-NonUtilisable  6.980546e+17  Ulrika   \n",
       "12823  Message-NonUtilisable  6.989351e+17  Ulrika   \n",
       "12824  Message-NonUtilisable  6.987924e+17  Ulrika   \n",
       "12825  Message-NonUtilisable  6.995217e+17  Ulrika   \n",
       "\n",
       "                                                    TEXT  nb_retweets  \\\n",
       "0      Inondations dans l'Aude:dégâts \"de l'ordre de ...    -0.785795   \n",
       "1      Département Hérault en vigilance Jaune pluie ️...     1.647024   \n",
       "2      [#CodesCourtage] Inondations tragiques dans l’...    -0.785795   \n",
       "3      Merci à @FDSEA77 pour ces photos prises par #d...     1.755725   \n",
       "4      [️ALERTE MÉTÉO⚠️] Vigilance #orange \"orages, p...     1.647024   \n",
       "...                                                  ...          ...   \n",
       "12821  J'aime une vidéo @YouTube de @davidlpokemon - ...    -0.785795   \n",
       "12822  Ce soir je dors en #Bretagne :-)\\nje dors en B...    -0.785795   \n",
       "12823  Tempête Ulrika sur le Bassin d'Arcachon Andern...    -0.785795   \n",
       "12824  Mi è piaciuto un video di @YouTube: http://you...    -0.785795   \n",
       "12825  Bretagne profonde https://youtu.be/jRRLH3IfszQ...    -0.785795   \n",
       "\n",
       "       nb_likes  num_tweets  following  followers     likes     lists  \\\n",
       "0     -0.894291    0.956674   1.387893   0.349577  1.078412 -0.657592   \n",
       "1      1.586924   -0.469193   1.005712   0.814903  1.892032  1.030431   \n",
       "2     -0.894291   -0.829161   0.098955   0.104824 -0.498565 -0.657592   \n",
       "3      1.741995   -0.165393   0.863903   1.192329  0.537415  1.030431   \n",
       "4      1.476086    0.560304   0.570291   1.373599  1.748246 -0.657592   \n",
       "...         ...         ...        ...        ...       ...       ...   \n",
       "12821 -0.894291   -0.255538  -1.511671  -3.149651 -0.687065 -0.657592   \n",
       "12822 -0.894291   -0.858333   1.028186  -0.244243 -0.386106  1.589166   \n",
       "12823 -0.894291    0.461589   1.187429  -0.015061 -0.138041 -0.657592   \n",
       "12824 -0.894291   -0.632697   1.381086  -0.168708 -0.173275 -0.657592   \n",
       "12825 -0.894291   -0.387825  -0.212842  -1.189493 -1.343739 -0.657592   \n",
       "\n",
       "                        CAT3                   CAT2  \\\n",
       "0         Message-InfoUrgent     Message-Utilisable   \n",
       "1         Message-InfoUrgent     Message-Utilisable   \n",
       "2         Message-InfoUrgent     Message-Utilisable   \n",
       "3      Message-InfoNonUrgent     Message-Utilisable   \n",
       "4         Message-InfoUrgent     Message-Utilisable   \n",
       "...                      ...                    ...   \n",
       "12821  Message-NonUtilisable  Message-NonUtilisable   \n",
       "12822  Message-NonUtilisable  Message-NonUtilisable   \n",
       "12823  Message-NonUtilisable  Message-NonUtilisable   \n",
       "12824  Message-NonUtilisable  Message-NonUtilisable   \n",
       "12825  Message-NonUtilisable  Message-NonUtilisable   \n",
       "\n",
       "                                              text_clean  \\\n",
       "0      inondations dans l'aude:dégâts \"de l'ordre de ...   \n",
       "1      département hérault en vigilance jaune pluie ️...   \n",
       "2      [#codescourtage] inondations tragiques dans l’...   \n",
       "3      merci à @fdseanumnum pour ces photos prises pa...   \n",
       "4      [️ale e météo⚠️] vigilance #orange \"orages, pl...   \n",
       "...                                                  ...   \n",
       "12821  j'aime une vidéo @youtube de @davidlpokemon - ...   \n",
       "12822  ce soir je dors en #bretagne :-) je dors en br...   \n",
       "12823  tempête ulrika sur le bassin d'arcachon andern...   \n",
       "12824  mi è piaciuto un video di @youtube:    tempête...   \n",
       "12825                  bretagne profonde    via @youtube   \n",
       "\n",
       "                                          processed_text  \n",
       "0      Inondations dans l Aude dégâts de l ordre de n...  \n",
       "1      Département Hérault en vigilance Jaune pluie ️...  \n",
       "2       CodesCourtage Inondations tragiques dans l Au...  \n",
       "3      Merci à pour ces photos prises par drones qui ...  \n",
       "4       ️ALERTE MÉTÉO⚠️ Vigilance orange orages pluie...  \n",
       "...                                                  ...  \n",
       "12821  J aime une vidéo de   Ouverture de numero Boos...  \n",
       "12822  Ce soir je dors en Bretagne je dors en Bretagn...  \n",
       "12823  Tempête Ulrika sur le Bassin d Arcachon Andern...  \n",
       "12824  Mi è piaciuto un video di   Tempête en Mer d I...  \n",
       "12825                           Bretagne profonde   via   \n",
       "\n",
       "[12826 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 847
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "PNP1f-OrehXK",
    "outputId": "24902c98-68b5-492f-9ee0-8509eddc17b6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "CPkgJxJddYhN"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#events_test= ['EffondrementMarseille']\n",
    "\n",
    "events_test= ['Bruno', 'Eleanor']\n",
    "\n",
    "df_test = df[df.event.isin(events_test)]\n",
    "\n",
    "df_train = df[~df.event.isin(events_test)]\n",
    "\n",
    "#df_train , df_test = train_test_split(df,random_state=1, test_size=0.2)\n",
    "\n",
    "#df_train=df_en[['text_clean','CAT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 847
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Fp3zA36i3Hmv",
    "outputId": "a244246a-bb52-4f29-f2a9-fb5015faa5eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nouveau risque d inondation en hongrie un village évacué'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['processed_text'].iloc[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "lmxzSGxF2y8u",
    "outputId": "320825e1-75c6-41fc-cb3f-df9968588952"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "9sctUMZj3y8Z",
    "outputId": "3e78bdad-f240-415a-d115-e2e910a0f030"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "2uM49TtI6LLN",
    "outputId": "2a4123e9-9923-4fd8-85b4-dde5c329ad10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Degats-Materiels', 1: 'Avertissement-conseil', 2: 'AutresMessages', 3: 'Message-NonUtilisable', 4: 'Soutiens', 5: 'Degats-Humains', 6: 'Critiques'}\n",
      "{0: 'Degats-Materiels', 1: 'Avertissement-conseil', 2: 'AutresMessages', 3: 'Message-NonUtilisable', 4: 'Soutiens', 5: 'Degats-Humains', 6: 'Critiques'}\n"
     ]
    }
   ],
   "source": [
    "def get_sentences_labels(df,text_column='text_clean',label_column='CAT',cat_labels=None):\n",
    "    dic_cat_labels = cat_labels if cat_labels is not None else {x:value for x,value in enumerate(df[label_column].unique())}\n",
    "    print(dic_cat_labels)\n",
    "    dic_labels_to_cat = {value:x for x,value in dic_cat_labels.items() }\n",
    "    #df[text_column]= df[text_column].map(lambda text_clean : re.sub('[\"#$%&()*+,-./:;<=>@[\\]^_`{|}~\\n\\t’\\']', '', text_clean))\n",
    "    df2 = df[label_column].map(dic_labels_to_cat)\n",
    "    sentences = df[text_column].values\n",
    "    labels = df2.values.astype(int)\n",
    "    return sentences,labels,dic_cat_labels\n",
    "\n",
    "def custom_sentences_labels(df,dic_cat,dic_cat2,text_column='text_clean',label_column='CAT'):\n",
    "    dic_cat_labe = dic_cat\n",
    "    dic_cat_labe2 = dic_cat2\n",
    "    #df['texte']= df['texte'].map(lambda text_clean : re.sub('[\"#$%&()*+,-./:;<=>@[\\]^_`{|}~\\n\\t’\\']', '', text_clean))\n",
    "    dic_labels_to_cat = {v: k for k, v in dic_cat_labe.items()}\n",
    "    dic_labels_to_cat2 = {v: k for k, v in dic_cat_labe2.items()}\n",
    "    sentences = df[text_column].values\n",
    "    df_cat = df['classe1'].map(dic_labels_to_cat)\n",
    "    labels_CAT = df_cat.values.astype(int)\n",
    "    df_cat2 = df['classe2'].map(dic_labels_to_cat2)\n",
    "    labels_CAT2 = df_cat2.values.astype(int)\n",
    "\n",
    "    return sentences,(labels_CAT,labels_CAT2),dic_cat_labe\n",
    "\n",
    "dic_cat_labels_CAT = {0: 'Poubelle', 1: 'UsageDetourne', 2: 'UsageMedical'}\n",
    "dic_cat_labels_CAT3 = {0: 'Poubelle', 1: 'opinionNegative', 2: 'opinionPositive',3:'sansOpinion-ou-mixte'}\n",
    "\n",
    "sentences_train,labels_train,dic_cat_labels=get_sentences_labels(df_train,text_column='processed_text',label_column='CAT')\n",
    "sentences_test,labels_test,dic_cat_labels=get_sentences_labels(df_test,text_column='processed_text',label_column='CAT',cat_labels=dic_cat_labels)\n",
    "\n",
    "#sentences_train,labels_train,_=custom_sentences_labels(df_train,dic_cat_labels_CAT,dic_cat_labels_CAT3,text_column='texte',label_column='CAT')\n",
    "#sentences_test,test_labels,_=custom_sentences_labels(df_test,dic_cat_labels_CAT,dic_cat_labels_CAT3,text_column='texte',label_column='CAT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "fL7hMw6rehXc",
    "outputId": "ba2a219c-c7b8-4193-9c3d-f3ae72217731"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "iHmNNf28ehXg",
    "outputId": "8b5ecdff-40d3-4e31-b460-05417e895c46"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'crisis_names = [\\'irma\\',\\'bruno\\',\\'aude\\',\\'harvey\\',\\'eleanor\\',\\'corse-fione\\',\\'beryl−guadeloupe\\',\\'corse\\',\\'egon\\',\\'susanna\\',\\'ulrika\\',\\'reunion−berguitta\\',\\'marseille\\',\\'effondrementmarseille\\',\\'guadeloupe\\',\\'corse\\',\\'immeuble\\',\\'martinique\\',\\'saint martin\\',\\'berguitta\\']\\ncrisis_scrap = [\\'marseille\\',\\'bruno\\',\\'crue\\', \\'crues\\', \\'aude\\', \\'carcassonne\\', \\'trèbes\\', \\'trebes\\',\\'corse\\', \\'corsica\\', \\'hautecorse\\', \\'haute-corse\\',\\'crue\\',\\'béryl\\', \\'beryl\\', \\'guadeloupe\\', \\'ondetropicale\\',\\'réunion\\', \\'reunion\\', \\'lareunion\\', \\'fakir\\', \\'laréunion\\',\\'réunion\\', \\'reunion\\', \\'lareunion\\',\\' berguitta\\',\\' laréunion\\',\\'corse\\', \\'fionn\\', \\'corsica\\', \\'ana\\',\\'irma\\',\\'ouraganIRMA\\', \\'saintmartin\\', \\'stmartin\\', \\'saintbarthelemy\\', \\'saintbarth\\', \\'stbarth\\',\\'harvey\\', \\'martinique\\', \\'guadeloupe\\',\\'egon\\',\\'ulrika\\', \\'vendée\\',\\'bretagne\\',\\'susanna\\']\\ncrisis_scrap=crisis_scrap+crisis_names\\nprint(crisis_scrap)\\nfor i in range(len(sentences_train)):\\n    big_regex = re.compile(\\'|\\'.join(map(re.escape, crisis_scrap)))\\n    sentences_train[i] = big_regex.sub(\" \", sentences_train[i])'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''crisis_names = ['irma','bruno','aude','harvey','eleanor','corse-fione','beryl−guadeloupe','corse','egon','susanna','ulrika','reunion−berguitta','marseille','effondrementmarseille','guadeloupe','corse','immeuble','martinique','saint martin','berguitta']\n",
    "crisis_scrap = ['marseille','bruno','crue', 'crues', 'aude', 'carcassonne', 'trèbes', 'trebes','corse', 'corsica', 'hautecorse', 'haute-corse','crue','béryl', 'beryl', 'guadeloupe', 'ondetropicale','réunion', 'reunion', 'lareunion', 'fakir', 'laréunion','réunion', 'reunion', 'lareunion',' berguitta',' laréunion','corse', 'fionn', 'corsica', 'ana','irma','ouraganIRMA', 'saintmartin', 'stmartin', 'saintbarthelemy', 'saintbarth', 'stbarth','harvey', 'martinique', 'guadeloupe','egon','ulrika', 'vendée','bretagne','susanna']\n",
    "crisis_scrap=crisis_scrap+crisis_names\n",
    "print(crisis_scrap)\n",
    "for i in range(len(sentences_train)):\n",
    "    big_regex = re.compile('|'.join(map(re.escape, crisis_scrap)))\n",
    "    sentences_train[i] = big_regex.sub(\" \", sentences_train[i])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "aSajUPDmehXl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "LBU62oKH6Sxu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wNjw7FVV6VYy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JwRAldx66eUH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fASheNBJEuHj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "h5wUaQJKLaXg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251,
     "referenced_widgets": [
      "f72eddb948294b7bb00463d4ed6cc5af",
      "a0d22be9c5af47fba9826f45cb404d14",
      "d6a4103ed28c441aaece9f8f9f11c8b5",
      "60831d0d88144f8aa18798597bf1ac94",
      "2024e77c10704e18bbb473afa0f5a908",
      "94aca90ab4d843b3a2e9e067a2b92395",
      "16aebfb576bc463db97e5a222b9f264f",
      "0c2259b7d4ad4020ab30a1a849287ccf",
      "846e2eb2279a47a39ab28c47fc60ab54",
      "3744741d6d7c478ea51f4ed86cfcd437",
      "ce9418995bda46fc9efd0f24996e8167",
      "44175059856346149d08ed289d4179c3",
      "e9819ed7b8b74bc4ab9c31edb3a8caae",
      "0d362c6a6c9245f4815c934ccaf1d717",
      "51295ca06b7b457baeb45d11c6e77b09",
      "7b3b7019567541cf99bfb59c3780325c"
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "fU3z3D5hNwnj",
    "outputId": "a21df7bc-f772-4ae0-a018-e7345d522bb1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from bertInput import BertInput\n",
    "\n",
    "bert_input= BertInput(AutoTokenizer.from_pretrained('flaubert-base-cased'))\n",
    "\n",
    "\n",
    "X_train = bert_input.fit_transform(sentences_train)\n",
    "X_test = bert_input.fit_transform(sentences_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "T-dNRe3T6jB-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11386"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GEededgf6pdP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Li9mOH97vqZu"
   },
   "outputs": [],
   "source": [
    "features = ['num_tweets','following','followers','likes','lists','nb_retweets','nb_likes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "2qWr_SoAvjyD"
   },
   "outputs": [],
   "source": [
    "# Use 90% for training and 10% for validation.\n",
    "train_inputs, validation_inputs, train_labels, validation_labels,train_features,validation_features,train_masks,validation_masks = train_test_split(X_train[0], labels_train,df_train[features].astype(float).values.tolist(),X_train[1],random_state=1, test_size=0.2)\n",
    "# Do the same for the masks.\n",
    "#train_masks, validation_masks= train_test_split(,random_state=1, test_size=0.2)\n",
    "\n",
    "test_inputs = X_test[0]\n",
    "test_masks = X_test[1]\n",
    "test_features = df_test[features].astype(float).values.tolist()\n",
    "test_labels = labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "e9EtZELWvQNq",
    "outputId": "5144d70e-584b-4e56-c27f-9dcd0945ab49"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "5MHONo0BEwRR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "QLbLpNo1EyRX"
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "# Convert all inputs and labels into torch tensors, the required datatype \n",
    "# for our model.\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "test_inputs = torch.tensor(test_inputs)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "test_masks = torch.tensor(test_masks)\n",
    "\n",
    "train_features = torch.tensor(train_features)\n",
    "validation_features = torch.tensor(validation_features)\n",
    "test_features = torch.tensor(test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "YCqZsxQG8qkL",
    "outputId": "aa567bec-4ba5-4be6-ba15-8c11c20178cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9108\n"
     ]
    }
   ],
   "source": [
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "N44YvMDrowB7"
   },
   "outputs": [],
   "source": [
    "def get_label_callback(dataset,idx):\n",
    "    return dataset[idx][3].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "EplrI9njoxJl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "iIawUkP_popj",
    "outputId": "ef5231b9-97df-4c4e-e6e3-7242d4675162"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "aDJeM2k1E02U"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs,train_masks,train_features,train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "#train_sampler = ImbalancedDatasetSampler(train_data,callback_get_label=get_label_callback)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size,drop_last=True )\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs,validation_masks ,validation_features,validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Create the DataLoader for our test set.\n",
    "test_data = TensorDataset(test_inputs,test_masks,test_features, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "dpLbwoAApnxJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ql6VZYido-w_"
   },
   "outputs": [],
   "source": [
    "#model =FlaubertBertForSequenceClassification.from_pretrained('bert_fine_tuned_all_fr',num_labels = 7)\n",
    "#model.bert.embeddings.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4f9cae1d99f3443fb98b00a4f7e682cf",
      "ff14b9e555c54d7c89c1dbbff543b47b",
      "77980552d8a344929721990a95e75f95",
      "06a769832535409e90bebadf4385bf56",
      "d73a2248948b4a8ba64d0c61d3c4a0e7",
      "ef456ebc1a3f459d8db57ffa12f509bd",
      "f6aadf099bc54f279d31aacca1e7ed89",
      "065edcd156034674802afe441b506472",
      "8e96be71b679486e89215ca965df9e9b",
      "273dc786cb2c49f29b2acb292eba3b9b",
      "fe09e9ce448e47f5baf8074e72c8f99f",
      "795050feb4c9465c80729c5184dbac43",
      "652b303e2fb648069359c59e57ff5367",
      "8cb852ff447a4dd5b106bf78f46fa11f",
      "bd9b36bf1b7040d59206b02525d8daaf",
      "c8615680e0404a47b76b3e9cf2c6b395"
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q-w7gOSsE3gg",
    "outputId": "36f4257e-2451-4b4a-bee9-81631d68e23f"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_model = AutoModel.from_pretrained(\"moumeneb1/flaubert-base-cased-ecology_crisis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BasicBertForClassification(\n",
       "  (bert): FlaubertModel(\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (embeddings): Embedding(68729, 768, padding_idx=2)\n",
       "    (layer_norm_emb): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (attentions): ModuleList(\n",
       "      (0): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): MultiHeadAttention(\n",
       "        (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm1): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (ffns): ModuleList(\n",
       "      (0): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): TransformerFFN(\n",
       "        (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (layer_norm2): ModuleList(\n",
       "      (0): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (1): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (2): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (3): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (4): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (5): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (6): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (7): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (8): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (9): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (10): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (11): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BasicBertForClassification(base_model,7)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "4OzL-JucAhxR"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score,recall_score,precision_score\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Function to calculate the f1_score of our predictions vs labels\n",
    "def flat_f1(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, pred_flat, average='macro')\n",
    "\n",
    "def flat_recall(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return recall_score(labels_flat, pred_flat, average='macro')\n",
    "\n",
    "def flat_precision(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return precision_score(labels_flat, pred_flat, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iA51FWSoS5na"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "W3pKjPg9Ivds"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW,get_linear_schedule_with_warmup\n",
    "\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 4\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * epochs \n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "yhDsewdSehZD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "qIABJxZ-19ii"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "qZAsjKnKI4Rx",
    "outputId": "b10dfd2d-35fb-47c6-9983-4b7a4cf615f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    569.    Elapsed: 0:00:08.\n",
      "  Batch    80  of    569.    Elapsed: 0:00:16.\n",
      "  Batch   120  of    569.    Elapsed: 0:00:24.\n",
      "  Batch   160  of    569.    Elapsed: 0:00:32.\n",
      "  Batch   200  of    569.    Elapsed: 0:00:40.\n",
      "  Batch   240  of    569.    Elapsed: 0:00:48.\n",
      "  Batch   280  of    569.    Elapsed: 0:00:55.\n",
      "  Batch   320  of    569.    Elapsed: 0:01:04.\n",
      "  Batch   360  of    569.    Elapsed: 0:01:12.\n",
      "  Batch   400  of    569.    Elapsed: 0:01:20.\n",
      "  Batch   440  of    569.    Elapsed: 0:01:28.\n",
      "  Batch   480  of    569.    Elapsed: 0:01:36.\n",
      "  Batch   520  of    569.    Elapsed: 0:01:44.\n",
      "  Batch   560  of    569.    Elapsed: 0:01:52.\n",
      "\n",
      "  Average training loss: 0.62\n",
      "  Training epcoh took: 0:01:54\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aboumada/anaconda3/envs/torch_tf/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/aboumada/anaconda3/envs/torch_tf/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "save currently the best model to [tmp]\n",
      "save model parameters to [tmp]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.83\n",
      "  F1: 0.62\n",
      "  Recall: 0.63\n",
      "  Precision: 0.63\n",
      "  Validation took: 0:00:09\n",
      "\n",
      "======== Epoch 2 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    569.    Elapsed: 0:00:08.\n",
      "  Batch    80  of    569.    Elapsed: 0:00:16.\n",
      "  Batch   120  of    569.    Elapsed: 0:00:24.\n",
      "  Batch   160  of    569.    Elapsed: 0:00:32.\n",
      "  Batch   200  of    569.    Elapsed: 0:00:41.\n",
      "  Batch   240  of    569.    Elapsed: 0:00:49.\n",
      "  Batch   280  of    569.    Elapsed: 0:00:57.\n",
      "  Batch   320  of    569.    Elapsed: 0:01:06.\n",
      "  Batch   360  of    569.    Elapsed: 0:01:14.\n",
      "  Batch   400  of    569.    Elapsed: 0:01:22.\n",
      "  Batch   440  of    569.    Elapsed: 0:01:30.\n",
      "  Batch   480  of    569.    Elapsed: 0:01:38.\n",
      "  Batch   520  of    569.    Elapsed: 0:01:46.\n",
      "  Batch   560  of    569.    Elapsed: 0:01:54.\n",
      "\n",
      "  Average training loss: 0.39\n",
      "  Training epcoh took: 0:01:56\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "save currently the best model to [tmp]\n",
      "save model parameters to [tmp]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Accuracy: 0.85\n",
      "  F1: 0.65\n",
      "  Recall: 0.65\n",
      "  Precision: 0.67\n",
      "  Validation took: 0:00:09\n",
      "\n",
      "======== Epoch 3 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    569.    Elapsed: 0:00:08.\n",
      "  Batch    80  of    569.    Elapsed: 0:00:16.\n",
      "  Batch   120  of    569.    Elapsed: 0:00:24.\n",
      "  Batch   160  of    569.    Elapsed: 0:00:32.\n",
      "  Batch   200  of    569.    Elapsed: 0:00:41.\n",
      "  Batch   240  of    569.    Elapsed: 0:00:49.\n",
      "  Batch   280  of    569.    Elapsed: 0:00:57.\n",
      "  Batch   320  of    569.    Elapsed: 0:01:05.\n",
      "  Batch   360  of    569.    Elapsed: 0:01:13.\n",
      "  Batch   400  of    569.    Elapsed: 0:01:21.\n",
      "  Batch   440  of    569.    Elapsed: 0:01:29.\n",
      "  Batch   480  of    569.    Elapsed: 0:01:37.\n",
      "  Batch   520  of    569.    Elapsed: 0:01:45.\n",
      "  Batch   560  of    569.    Elapsed: 0:01:53.\n",
      "\n",
      "  Average training loss: 0.26\n",
      "  Training epcoh took: 0:01:55\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.84\n",
      "  F1: 0.64\n",
      "  Recall: 0.65\n",
      "  Precision: 0.66\n",
      "  Validation took: 0:00:07\n",
      "\n",
      "======== Epoch 4 / 4 ========\n",
      "Training...\n",
      "  Batch    40  of    569.    Elapsed: 0:00:08.\n",
      "  Batch    80  of    569.    Elapsed: 0:00:16.\n",
      "  Batch   120  of    569.    Elapsed: 0:00:24.\n",
      "  Batch   160  of    569.    Elapsed: 0:00:32.\n",
      "  Batch   200  of    569.    Elapsed: 0:00:40.\n",
      "  Batch   240  of    569.    Elapsed: 0:00:48.\n",
      "  Batch   280  of    569.    Elapsed: 0:00:56.\n",
      "  Batch   320  of    569.    Elapsed: 0:01:04.\n",
      "  Batch   360  of    569.    Elapsed: 0:01:12.\n",
      "  Batch   400  of    569.    Elapsed: 0:01:20.\n",
      "  Batch   440  of    569.    Elapsed: 0:01:28.\n",
      "  Batch   480  of    569.    Elapsed: 0:01:36.\n",
      "  Batch   520  of    569.    Elapsed: 0:01:44.\n",
      "  Batch   560  of    569.    Elapsed: 0:01:52.\n",
      "\n",
      "  Average training loss: 0.17\n",
      "  Training epcoh took: 0:01:54\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.83\n",
      "  F1: 0.63\n",
      "  Recall: 0.65\n",
      "  Precision: 0.65\n",
      "  Validation took: 0:00:07\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_noFeatures(model,train_dataloader,validation_dataloader,epochs,torch.device('cuda'),optimizer,scheduler,criterion,writer)\n",
    "print(\"\")\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.add_text('Train', 'This is an lstm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "KzmtPO6OJbcG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels for 90 test sentences...\n",
      "    DONE.\n"
     ]
    }
   ],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "print('Predicting labels for {:,} test sentences...'.format(len(test_dataloader)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "\n",
    "# Tracking variables \n",
    "predictions_cat,predictions_cat3,predictions_cat2 , true_labels_cat,true_labels_cat2  = [], [],[],[],[]\n",
    "\n",
    "# Predict \n",
    "for batch in test_dataloader:\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(torch.device(\"cuda\")) for t in batch)\n",
    "\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask,b_features,b_labels_cat = batch\n",
    "\n",
    "    # Telling the model not to compute or store gradients, saving memory and \n",
    "    # speeding up prediction\n",
    "    with torch.no_grad():\n",
    "        # Forward pass, calculate logit predictions\n",
    "        outputs = model((b_input_ids,b_input_mask,b_features))\n",
    "        logits_cat = outputs[0]\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits_cat = logits_cat.detach().cpu().numpy()\n",
    "    label_ids_cat = b_labels_cat.to('cpu').numpy()\n",
    "    predictions_cat.extend(logits_cat)\n",
    "    true_labels_cat.extend(label_ids_cat)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "print('    DONE.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "n44qF6NPKHcv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.86875\n",
      "                       precision    recall  f1-score   support\n",
      "\n",
      "       AutresMessages     0.0811    0.2000    0.1154        15\n",
      "Avertissement-conseil     0.6983    0.8571    0.7696       189\n",
      "       Degats-Humains     0.6000    0.7778    0.6774        27\n",
      "     Degats-Materiels     0.4773    0.7000    0.5676        30\n",
      "Message-NonUtilisable     0.9595    0.8861    0.9213      1176\n",
      "             Soutiens     0.3333    0.6667    0.4444         3\n",
      "\n",
      "             accuracy                         0.8688      1440\n",
      "            macro avg     0.5249    0.6813    0.5826      1440\n",
      "         weighted avg     0.8980    0.8688    0.8801      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred_flat_cat = np.argmax(predictions_cat, axis=1)\n",
    "true_labels_cat=[dic_cat_labels.get(x) for x in true_labels_cat]\n",
    "pred_flat_cat = [dic_cat_labels.get(x) for x in pred_flat_cat]\n",
    "\n",
    "\n",
    "cr= classification_report(true_labels_cat,pred_flat_cat,digits=4)\n",
    "print(accuracy_score(pred_flat_cat,true_labels_cat))\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "jF21fm-zehZd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "save model parameters to [Crisis_Binary_flaubert_base.pth]\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Crisis_Binary_flaubert_base.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Pycho_sentiment_bert_adepted.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-c4c5d2e6790e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBasicBertForClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pycho_sentiment_bert_adepted.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PFE/nlpcrisis/Codes/deep_learning/my_models/models.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msaved\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'args'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mbert_base_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bert_config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_tf/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_tf/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch_tf/lib/python3.7/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Pycho_sentiment_bert_adepted.pth'"
     ]
    }
   ],
   "source": [
    "model = BasicBertForClassification.load(\"Pycho_sentiment_bert_adepted.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "HEIPOlO8ehZh"
   },
   "outputs": [],
   "source": [
    "tokenizer = ATokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "sentences = df[\"texte\"]\n",
    "bert_input = BertInput(tokenizer)\n",
    "sentences = bert_input.fit_transform(sentences)\n",
    "input_ID = torch.tensor(sentences[0])\n",
    "input_MASK = torch.tensor(sentences[1])\n",
    "print(len(sentences))\n",
    "\n",
    "input_ID = torch.tensor(sentences[0])\n",
    "input_MASK = torch.tensor(sentences[1])\n",
    "\n",
    "tensor_dataset = TensorDataset(\n",
    "    input_ID, input_MASK)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    tensor_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "pred = []\n",
    "for index, batch in enumerate(dataloader):\n",
    "    output = model(batch)\n",
    "    label_index = np.argmax(output[0].cpu().detach().numpy())\n",
    "    print(index)\n",
    "    pred.append(labels_dict.get(label_index))\n",
    "df['prediction'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "MxFV4TGAehZl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "60S-GzCfehZp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "name_to_cat = {v:k for k,v in dic_cat_labels_CAT.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "liste = []\n",
    "for _,row in df_test.iterrows():\n",
    "    x_input = []\n",
    "    x_input = berts_input([row['text_clean']],BertTokenizer.from_pretrained('bert-base-multilingual-cased'))\n",
    "    x_input = torch.tensor(x_input)\n",
    "    x_input = x_input.to(torch.device('cuda'))\n",
    "    output = model(x_input)\n",
    "    output[0].cpu\n",
    "    output[0].cpu()\n",
    "    pred_flat = np.argmax(output[0].cpu().detach().numpy())\n",
    "    if pred_flat!=name_to_cat[row['CAT']]:\n",
    "        print('Annotation human:',row['CAT'])\n",
    "        print('Annotation Machine:',dic_cat_labels_CAT[pred_flat] )\n",
    "        print(row['TEXT'])\n",
    "        liste.append([row['TEXT'],row['CAT'],dic_cat_labels_CAT[pred_flat]])\n",
    "        print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_resultat = pd.DataFrame(liste,columns=['Text','Human annotation','Machine Annotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_resultat.to_csv('error.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "YbFw5Bf6ehZv"
   },
   "outputs": [],
   "source": [
    "x_input = torch.tensor(x_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "yDaGb1nWQ524"
   },
   "outputs": [],
   "source": [
    "# Three classes\n",
    "x_input = x_input.to(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "KQSp-xzlehZ5"
   },
   "outputs": [],
   "source": [
    "output = model(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "AQ2VdzKyRVzv"
   },
   "outputs": [],
   "source": [
    "output[0].cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "eS08J490RpG4"
   },
   "outputs": [],
   "source": [
    "pred_flat = np.argmax(output[0].cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "-koMpoCqehaH"
   },
   "outputs": [],
   "source": [
    "pred_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "2br-uW8pehaN"
   },
   "outputs": [],
   "source": [
    "x=df_test['text_clean'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kqT-7s0ehaS"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "xquzubrQehaU"
   },
   "outputs": [],
   "source": [
    "max_len = max(len(s) for s in sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "WaNyf9f5ehaZ"
   },
   "outputs": [],
   "source": [
    "from models import BasicBertForClassification, BertFeaturesForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# When you add a model or a domain to your app Just import your model and add the path to it\n",
    "models_dic = {\n",
    "    \"crisis_binary\": {\n",
    "        \"bert_base_cased\": {\n",
    "            \"model\": BasicBertForClassification,\n",
    "            \"path\": \"dqdsqdq\",\n",
    "        },\n",
    "        \"flaubert_base_cased\": {\n",
    "            \"model\": BasicBertForClassification,\n",
    "            \"path\": \"dqshdbjq\"\n",
    "        },\n",
    "        \"flaubert_base_features\": {\n",
    "            \"model\": BertFeaturesForSequenceClassification,\n",
    "            \"path\": \"dqshdbjq\",\n",
    "            \"features\": ['nbretweet', 'nblike']\n",
    "        },\n",
    "        \"labels_dic\": {\n",
    "            0: 'Message-Utilisable',\n",
    "            1: 'Message-NonUtilisable'\n",
    "        }\n",
    "    },\n",
    "    \"crisis_Three_Class\": {\n",
    "        \"bert_base_cased\": {\n",
    "            \"model\": BasicBertForClassification,\n",
    "            \"path\": \"dqdsqdq\"\n",
    "        },\n",
    "        \"flaubert_base_cased\": {\n",
    "            \"model\": BasicBertForClassification,\n",
    "            \"path\": \"dqshdbjq\"\n",
    "        },\n",
    "        \"flaubert_base_features\": {\n",
    "            \"model\": BasicBertForClassification,\n",
    "            \"path\": \"dqshdbjq\",\n",
    "            \"fatures\": [\"dqsdqsd\", \"\"]\n",
    "        },\n",
    "        \"labels_dic\": {\n",
    "            0: 'Message-InfoUrgent',\n",
    "            1: 'Message-InfoNonUrgent',\n",
    "            2: 'Message-NonUtilisable'}\n",
    "    },\n",
    "    \"crisis_MultiClass\": {\n",
    "        \"bert_base_cased\": {\n",
    "            \"model\": BasicBertForClassification,\n",
    "            \"path\": \"dqdsqdq\"\n",
    "        },\n",
    "        \"flaubert_base_cased\": {\n",
    "            \"model\": BasicBertForClassification,\n",
    "            \"path\": \"dqshdbjq\"\n",
    "        },\n",
    "        \"flaubert_base_features\": {\n",
    "            \"model\": BasicBertForClassification,\n",
    "            \"path\": \"dqshdbjq\",\n",
    "            \"fatures\": [\"dqsdqsd\", \"\"]\n",
    "        },\n",
    "        \"labels_dic\": {\n",
    "            0: 'Degats-Materiels',\n",
    "            1: 'Avertissement-conseil',\n",
    "            2: 'AutresMessages',\n",
    "            3: 'Message-NonUtilisable',\n",
    "            4: 'Soutiens',\n",
    "            5: 'Degats-Humains',\n",
    "            6: 'Critiques'}\n",
    "    },\n",
    "    \"psycho_sentiment\": {\n",
    "        \"bert_base_cased\": {\n",
    "            \"model\": BasicBertForClassification,\n",
    "            \"path\": \"dqdsqdq\"\n",
    "        },\n",
    "        \"flaubert_adapted_features\": {\n",
    "            \"model\": BertFeaturesForSequenceClassification,\n",
    "            \"path\": \"flaubert_classification.pth\",\n",
    "            \"tokenizer_base\" : \"flaubert-base-cased\",\n",
    "            \"features\" : ['nbretweet','nblike'],\n",
    "        },\n",
    "        \"labels_dic\":{\n",
    "            0: 'opinionNegative', \n",
    "            1: 'sansOpinion-ou-mixte', \n",
    "            2: 'opinionPositive'}\n",
    "    },\n",
    "    \"psycho_use\": {\n",
    "        \"model\": BasicBertForClassification,\n",
    "        \"path\": \",qldks,qdl\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def get_model(domain, model_name):\n",
    "    model = models_dic[domain][model_name][\"model\"].load(\n",
    "        models_dic[domain][model_name][\"path\"])\n",
    "\n",
    "    if \"features\" in models_dic[domain][model_name]:\n",
    "        features = models_dic[domain][model_name][\"features\"]\n",
    "    else:\n",
    "        features = []\n",
    "    Tokenizer = AutoTokenizer.from_pretrained(\n",
    "        models_dic[domain][model_name][\"tokenizer_base\"])\n",
    "\n",
    "    return model, Tokenizer, models_dic[domain][\"labels_dic\"], features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "BmEQaWevehae"
   },
   "outputs": [],
   "source": [
    "\n",
    "featuresExtrator = FeaturesExtraction(df, \"texte\")\n",
    "featuresExtrator.fit_transform()\n",
    "\n",
    "# Preprocessing\n",
    "text_preprocessing = TextPreprocessing(df, \"texte\")\n",
    "text_preprocessing.fit_transform()\n",
    "\n",
    "# Load model ,Tokenizer , labels_dict , features\n",
    "\n",
    "model, tokenizer, labels_dict, features = get_model(\n",
    "    \"psycho_sentiment\", \"flaubert_adapted_features\")\n",
    "\n",
    "print(features)\n",
    "# get text\n",
    "sentences = df_test[\"processed_text\"]\n",
    "bert_input = BertInput(tokenizer)\n",
    "sentences = bert_input.fit_transform(sentences)\n",
    "input_ID = torch.tensor(sentences[0])\n",
    "input_MASK = torch.tensor(sentences[1])\n",
    "print(len(sentences))\n",
    "if features:\n",
    "    features_column = df_test[features].values.astype(float).tolist()\n",
    "    features_column = torch.tensor(features_column)\n",
    "    tensor_dataset = TensorDataset(input_ID,input_MASK,features_column)\n",
    "else:\n",
    "    tensor_dataset = TensorDataset(sentences)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    tensor_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create an empty list\n",
    "sample_list = []\n",
    "# Iterate over sequence of numbers from 0 to 9\n",
    "for i in range(10):\n",
    "    # Append each number at the end of list\n",
    "    sample_list.append(i)\n",
    "sample_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "6xSARQ_aehak"
   },
   "outputs": [],
   "source": [
    "pred = []\n",
    "for index,batch in enumerate(dataloader):\n",
    "    output = model(batch)\n",
    "    label_index = np.argmax(output[0].cpu().detach().numpy())\n",
    "    pred.append(labels_dict.get(label_index))\n",
    "    df_test['predictions']=pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "mU_CQrvaehap"
   },
   "outputs": [],
   "source": [
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "XDXm44CXehas"
   },
   "outputs": [],
   "source": [
    "liste = []\n",
    "for _,row in df_test.iterrows():\n",
    "    x_input = []\n",
    "    x_input = bert_input.fit_transform([row['processed_text']])\n",
    "    input_ID = torch.tensor(x_input[0][0])\n",
    "    input_MASK = torch.tensor(x_input[1][0])\n",
    "    features_c = row[features].values.astype(float).tolist()\n",
    "    #features_c = torch.tensor(features_column)\n",
    "    output = model((input_ID,input_MASK,features_c,))\n",
    "    output[0].cpu\n",
    "    output[0].cpu()\n",
    "    pred_flat = np.argmax(output[0].cpu().detach().numpy())\n",
    "    print(pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "O8vvdluSehav"
   },
   "outputs": [],
   "source": [
    "sentences_text= df_test['processed_text']\n",
    "sentences_text.iloc[120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "tyl43ZmLehax"
   },
   "outputs": [],
   "source": [
    "test_labels[29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "QNI95yNKeha1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Z3-BA_vfeha4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "WnvSgbO0eha7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "LuWeeaeEeha9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": false,
    "id": "DaB8pXo6ehbC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "2u4UInmFehbG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Y1_tjR61ehbK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "1ObqrYFkehbN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AVpM54y3ehbQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "B3UFEv2cehbT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Rr60uzFcehbW"
   },
   "source": [
    "#### "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Flaubert_Multitask+learning+All.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "torch_tf_env",
   "language": "python",
   "name": "torch_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
